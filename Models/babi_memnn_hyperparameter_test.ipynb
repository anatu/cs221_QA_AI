{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import os as os\n",
    "import json\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers, callbacks, models\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from datetime import datetime\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        if int(nid) == 1: story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            substory = [[str(i)+\":\"]+x for i,x in enumerate(story) if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else: story.append(tokenize(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f):\n",
    "    data = parse_stories(f.readlines())\n",
    "    return [(story, q, answer) for story, q, answer in data]\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    X = []; Xq = []; Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [[word_idx[w] for w in s] for s in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        y = [word_idx[answer]]\n",
    "        X.append(x); Xq.append(xq); Y.append(y)\n",
    "    return ([pad_sequences(x, maxlen=story_maxlen) for x in X],\n",
    "            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n",
    "\n",
    "def do_flatten(el): \n",
    "    return isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes))\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if do_flatten(el): yield from flatten(el)\n",
    "        else: yield el\n",
    "            \n",
    "def stack_inputs(inputs):\n",
    "    for i,it in enumerate(inputs):\n",
    "        inputs[i] = np.concatenate([it, \n",
    "                           np.zeros((story_maxsents-it.shape[0],story_maxlen), 'int')])\n",
    "    return np.stack(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 50, 100, 100\n"
     ]
    }
   ],
   "source": [
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 40\n",
    "MODEL_NAME = \"LSTM_SelfAttention\"\n",
    "# Regularization parameter\n",
    "LAMBDA = 0.02\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
    "                           'babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n",
    "          '.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "challenge = []\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa7_counting_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa8_lists-sets_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa9_simple-negation_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa10_indefinite-knowledge_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa11_basic-coreference_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa12_conjunction-fact_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa13_compound-coreference_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa14_time-reasoning_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa15_basic-deduction_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa16_basic-induction_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa17_positional-reasoning_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa18_size-reasoning_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa19_path-finding_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa20_agents-motivations_{}.txt')\n",
    "# Default QA1 with 1000 samples\n",
    "#challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
    "# QA1 with 10,000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n",
    "# QA2 with 1000 samples\n",
    "#challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt'\n",
    "# QA2 with 10,000 samples\n",
    "#challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'\n",
    "#challenge = 'tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_{}.txt'\n",
    "\n",
    "\n",
    "#with tarfile.open(path) as tar:\n",
    "    #train_stories = get_stories(tar.extractfile(challenge[1].format('train')))\n",
    "    #test_stories = get_stories(tar.extractfile(challenge[1].format('test')))\n",
    "#np.random.shuffle(train_stories)\n",
    "#np.random.shuffle(test_stories)\n",
    "\n",
    "#stories = train_stories + test_stories\n",
    "\n",
    "#story_maxlen = max((len(s) for x, _, _ in stories for s in x))\n",
    "#story_maxsents = max((len(x) for x, _, _ in stories))\n",
    "#query_maxlen = max(len(x) for _, x, _ in stories)\n",
    "\n",
    "#vocab = sorted(set(flatten(stories)))\n",
    "#vocab.insert(0, '<PAD>')\n",
    "#vocab_size = len(vocab)\n",
    "\n",
    "#word_idx = dict((c, i) for i, c in enumerate(vocab))\n",
    "\n",
    "#inputs_train, queries_train, answers_train = vectorize_stories(train_stories, \n",
    "     #word_idx, story_maxlen, query_maxlen)\n",
    "#inputs_test, queries_test, answers_test = vectorize_stories(test_stories, \n",
    "     #word_idx, story_maxlen, query_maxlen)\n",
    "\n",
    "#inputs_train = stack_inputs(inputs_train)\n",
    "#inputs_test = stack_inputs(inputs_test)\n",
    "\n",
    "#inps = [inputs_train, queries_train]\n",
    "#val_inps = [inputs_test, queries_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py36/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n",
      "/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:136: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9500 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "1696/9500 [====>.........................] - ETA: 9s - loss: 2.1007 - acc: 0.1692 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-31d04d887d83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 hist=answer.fit(inps, answers_train, nb_epoch=20, batch_size=32,\n\u001b[0;32m--> 136\u001b[0;31m                            validation_split=0.05)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 loss, acc = answer.evaluate([inputs_test, queries_test], answers_test,\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emb_dim = 30\n",
    "reg1 = 0.002\n",
    "reg2 = 0.0000\n",
    "dropout1 = 0.3\n",
    "\n",
    "\n",
    "#regv1 = [0.01]\n",
    "#regv2 = [0.01]\n",
    "#dropv = [0.01]\n",
    "\n",
    "regv1 = [0.000, 0.01, 0.05, 0.1]\n",
    "regv2 = [0.000, 0.01, 0.05, 0.1]\n",
    "dropv = [0.000, 0.05, 0.1, 0.4]\n",
    "learn_rate = [5e-3]\n",
    "\n",
    "\n",
    "def emb_sent_bow(inp, reg):\n",
    "        emb_op = layers.TimeDistributed(Embedding(vocab_size, emb_dim, embeddings_regularizer=regularizers.l2(reg)))\n",
    "        emb = emb_op(inp)\n",
    "        emb = layers.Dropout(0.1)(emb)\n",
    "        emb = layers.Lambda(lambda x: K.sum(x, 2))(emb)\n",
    "        #     return Elemwise(0, False)(emb), emb_op\n",
    "        return emb, emb_op\n",
    "\n",
    "def one_hop(u, A, reg, dropout):\n",
    "    C, _ = emb_sent_bow(inp_story, reg)\n",
    "    x = layers.Reshape((1, emb_dim))(u)\n",
    "    x = layers.Dot(axes=2)([A, x])\n",
    "    x = layers.Reshape((story_maxsents,))(x)\n",
    "    x = layers.Activation('softmax')(x)\n",
    "    match = layers.Reshape((story_maxsents,1))(x)\n",
    "\n",
    "    x = layers.Dot(axes=1)([match, C])\n",
    "    x = layers.Reshape((emb_dim,))(x)\n",
    "    x = h(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Add()([x, emb_q])\n",
    "    return x, C\n",
    "    \n",
    "    \n",
    "\n",
    "#hist=answer.fit(inps, answers_train, nb_epoch=1, batch_size=32, validation_split=0.05)\n",
    "\n",
    "\n",
    "def build_model(reg1,reg2,dropout1):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #K.clear_session()\n",
    "    inp_story = layers.Input((story_maxsents, story_maxlen))\n",
    "    inp_q = layers.Input((query_maxlen,))\n",
    "    emb_story, emb_story_op = emb_sent_bow(inp_story, reg1)\n",
    "    emb_q = emb_story_op.layer(inp_q)\n",
    "    emb_q = layers.Lambda(lambda x: K.sum(x, 1))(emb_q)\n",
    "    h = layers.Dense(emb_dim, kernel_regularizer=regularizers.l2(reg2))\n",
    "    response, emb_story = one_hop(emb_q, emb_story, reg1, dropout1)\n",
    "    response, emb_story = one_hop(response, emb_story, reg1, dropout1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#answer = {}\n",
    "    \n",
    "f = open(\"hp_results.txt\", \"w+\")\n",
    "\n",
    "#j = 0\n",
    "#k = 0\n",
    "\n",
    "for c in range(len(challenge)):\n",
    "\n",
    "    with tarfile.open(path) as tar:\n",
    "        train_stories = get_stories(tar.extractfile(challenge[c].format('train')))\n",
    "        test_stories = get_stories(tar.extractfile(challenge[c].format('test')))\n",
    "    np.random.shuffle(train_stories)\n",
    "    np.random.shuffle(test_stories)\n",
    "\n",
    "    stories = train_stories + test_stories\n",
    "\n",
    "    story_maxlen = max((len(s) for x, _, _ in stories for s in x))\n",
    "    story_maxsents = max((len(x) for x, _, _ in stories))\n",
    "    query_maxlen = max(len(x) for _, x, _ in stories)\n",
    "\n",
    "    vocab = sorted(set(flatten(stories)))\n",
    "    vocab.insert(0, '<PAD>')\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word_idx = dict((c, i) for i, c in enumerate(vocab))\n",
    "\n",
    "    inputs_train, queries_train, answers_train = vectorize_stories(train_stories, \n",
    "         word_idx, story_maxlen, query_maxlen)\n",
    "    inputs_test, queries_test, answers_test = vectorize_stories(test_stories, \n",
    "         word_idx, story_maxlen, query_maxlen)\n",
    "\n",
    "    inputs_train = stack_inputs(inputs_train)\n",
    "    inputs_test = stack_inputs(inputs_test)\n",
    "\n",
    "    inps = [inputs_train, queries_train]\n",
    "    val_inps = [inputs_test, queries_test]\n",
    "\n",
    "    best_acc = 0\n",
    "    best_loss = 0\n",
    "    best_reg1 = 0\n",
    "    best_reg2 = 0\n",
    "    best_drop = 0\n",
    "    best_lr = 0\n",
    "\n",
    "    for i in range(len(regv1)):\n",
    "        #for j in range(len(regv2)):\n",
    "        for k in range(len(dropv)):\n",
    "            for l in range(len(learn_rate)):\n",
    "                inp_story = layers.Input((story_maxsents, story_maxlen))\n",
    "                inp_q = layers.Input((query_maxlen,))\n",
    "                emb_story, emb_story_op = emb_sent_bow(inp_story, regv1[i])\n",
    "                emb_q = emb_story_op.layer(inp_q)\n",
    "                emb_q = layers.Lambda(lambda x: K.sum(x, 1))(emb_q)\n",
    "                h = layers.Dense(emb_dim, kernel_regularizer=regularizers.l2(regv2[i]))\n",
    "\n",
    "                response, emb_story = one_hop(emb_q, emb_story, reg1, dropout1)\n",
    "                response, emb_story = one_hop(response, emb_story, reg1, dropout1)\n",
    "                # response, emb_story = one_hop(response, emb_story)\n",
    "                res = layers.Dense(vocab_size, activation='softmax')(response)\n",
    "                answer = Model([inp_story, inp_q], res)\n",
    "                answer.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "                               metrics=['accuracy'])\n",
    "                K.set_value(answer.optimizer.lr, learn_rate[l])\n",
    "\n",
    "                build_model(regv1[i],regv2[i],dropv[i])\n",
    "\n",
    "                res = layers.Dense(vocab_size, activation='softmax')(response)\n",
    "                answer = Model([inp_story, inp_q], res)\n",
    "                answer.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "                               metrics=['accuracy'])\n",
    "                K.set_value(answer.optimizer.lr, learn_rate[l])\n",
    "                hist=answer.fit(inps, answers_train, nb_epoch=20, batch_size=32,\n",
    "                           validation_split=0.05)\n",
    "\n",
    "                loss, acc = answer.evaluate([inputs_test, queries_test], answers_test,\n",
    "                                   batch_size=BATCH_SIZE)\n",
    "\n",
    "                #loss.append(loss)\n",
    "                #accv.append(acc)\n",
    "\n",
    "                print(challenge[c])\n",
    "                print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n",
    "                prstr = 'Reg1 = ' + str(regv1[i]) + ' Reg2 = ' + str(regv2[i]) + ' Dropout = ' + str(dropv[k]) + ' Learn Rate = ' + str(learn_rate[l])\n",
    "                print(prstr)\n",
    "\n",
    "\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_loss = loss\n",
    "                    best_reg1 = regv1[i]\n",
    "                    best_reg2 = regv2[i] \n",
    "                    best_drop = dropv[k]\n",
    "                    best_lr = learn_rate[l]\n",
    "\n",
    "                    filestr = prstr + '\\n' + 'Test loss = ' + str(round(loss,4)) + ' Test accuracy = ' + str(round(acc,4)) + '\\n'\n",
    "                    f.write(filestr)\n",
    "\n",
    "                if acc == 1:\n",
    "                    break\n",
    "\n",
    "                #prstr = 'Best Acc = ' + str(best_acc) + ' Best Reg1 = ' + str(best_reg1) + ' Best Reg2 = ' + str(best_reg2) + ' Best Drop = ' + str(best_drop) + ' Best Learn Rate = ' + str(learn_rate[l])\n",
    "                #print(prstr)\n",
    "\n",
    "                K.clear_session()\n",
    "            if acc == 1:\n",
    "                break\n",
    "        if acc == 1:\n",
    "            break\n",
    "\n",
    "    final_filestr = '\\n\\n\\n---------------------------------------------- QA'+str(c+1)+' ----------------------------------------------\\n'\n",
    "    final_filestr = final_filestr + '-------------------------------------------- Results ----------------------------------------------\\n\\n'\n",
    "    final_filestr = final_filestr + 'Best Acc = ' + str(round(best_acc,4)) + ' Best Loss = ' + str(round(best_loss,4)) + ' Best Reg1 = ' + str(best_reg1) + ' Best Reg2 = ' + str(best_reg2) + ' Best Drop = ' + str(best_drop) + ' Best Learn Rate = ' + str(learn_rate[l]) + '\\n\\n'\n",
    "    final_filestr = final_filestr + '-------------------------------------------- Results ----------------------------------------------\\n'\n",
    "    final_filestr = final_filestr + '---------------------------------------------- QA'+str(c+1)+' ----------------------------------------------\\n\\n\\n'\n",
    "\n",
    "\n",
    "    f.write(final_filestr)\n",
    "\n",
    "#beststr = 'Best Results:\\n'\n",
    "#beststr = beststr + 'Reg1 = ' + str(best_reg1) + ' Reg2 = ' + str(best_reg2) + ' Dropout = ' + str(best_drop) + ' Learn Rate = ' + str(best_lr) + '\\n' + ' Test loss = ' + str(best_loss) + ' Test accuracy = ' + str(best_acc)\n",
    "#f.write(beststr)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
