{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from __future__ import print_function\nfrom functools import reduce\nimport re\nimport tarfile\n\nimport numpy as np\nimport os as os\n\nfrom keras.utils.data_utils import get_file\nfrom keras.layers.embeddings import Embedding\nfrom keras import layers\nfrom keras.layers import recurrent\nfrom keras.models import Model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\n\n\nfrom keras.layers import Dot\nfrom keras.layers import Add\nfrom keras.layers import Flatten\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import regularizers\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29516f462cc53d690baa7901115f2d1439c2680c"
      },
      "cell_type": "code",
      "source": "def tokenize(sent):\n    '''Return the tokens of a sentence including punctuation.\n    >>> tokenize('Bob dropped the apple. Where is the apple?')\n    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n    '''\n    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n\n\ndef parse_stories(lines, only_supporting=False):\n    '''Parse stories provided in the bAbi tasks format\n    If only_supporting is true,\n    only the sentences that support the answer are kept.\n    '''\n    data = []\n    story = []\n    for line in lines:\n        line = line.decode('utf-8').strip()\n        nid, line = line.split(' ', 1)\n        nid = int(nid)\n        if nid == 1:\n            story = []\n        if '\\t' in line:\n            q, a, supporting = line.split('\\t')\n            q = tokenize(q)\n            if only_supporting:\n                # Only select the related substory\n                supporting = map(int, supporting.split())\n                substory = [story[i - 1] for i in supporting]\n            else:\n                # Provide all the substories\n                substory = [x for x in story if x]\n            data.append((substory, q, a))\n            story.append('')\n        else:\n            sent = tokenize(line)\n            story.append(sent)\n    return data\n\n\ndef get_stories(f, only_supporting=False, max_length=None):\n    '''Given a file name, read the file, retrieve the stories,\n    and then convert the sentences into a single story.\n    If max_length is supplied,\n    any stories longer than max_length tokens will be discarded.\n    '''\n    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n    flatten = lambda data: reduce(lambda x, y: x + y, data)\n    data = [(flatten(story), q, answer) for story, q, answer in data\n            if not max_length or len(flatten(story)) < max_length]\n    return data\n\n\ndef vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n    xs = []\n    xqs = []\n    ys = []\n    for story, query, answer in data:\n        x = [word_idx[w] for w in story]\n        xq = [word_idx[w] for w in query]\n        # let's not forget that index 0 is reserved\n        y = np.zeros(len(word_idx) + 1)\n        y[word_idx[answer]] = 1\n        xs.append(x)\n        xqs.append(xq)\n        ys.append(y)\n    return (pad_sequences(xs, maxlen=story_maxlen),\n            pad_sequences(xqs, maxlen=story_maxlen), np.array(ys))\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "455ca984d01c1f09c315400e1d2168a39faba8d5"
      },
      "cell_type": "code",
      "source": "RNN = recurrent.LSTM\nEMBED_HIDDEN_SIZE = 50\nSENT_HIDDEN_SIZE = 100\nQUERY_HIDDEN_SIZE = 100\nBATCH_SIZE = 128\nEPOCHS = 1000\nprint('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n                                                           EMBED_HIDDEN_SIZE,\n                                                           SENT_HIDDEN_SIZE,\n                                                           QUERY_HIDDEN_SIZE))\n\ntry:\n    path = get_file('babi-tasks-v1-2.tar.gz',\n                    origin='https://s3.amazonaws.com/text-datasets/'\n                           'babi_tasks_1-20_v1-2.tar.gz')\nexcept:\n    print('Error downloading dataset, please download it manually:\\n'\n          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n          '.tar.gz\\n'\n          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n    raise\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "023a2686fbf1f65ec49448d1cdbc033aaeb92f09"
      },
      "cell_type": "code",
      "source": "# challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n# QA1 with 10,000 samples\n# challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n# QA2 with 1000 samples\nchallenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt'\n# QA2 with 10,000 samples\n# challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'\nwith tarfile.open(path) as tar:\n    train = get_stories(tar.extractfile(challenge.format('train')))\n    test = get_stories(tar.extractfile(challenge.format('test')))\nnp.random.shuffle(train)\nnp.random.shuffle(test)\nvocab = set()\nfor story, q, answer in train + test:\n    vocab |= set(story + q + [answer])\nvocab = sorted(vocab)\n\n# Reserve 0 for masking via pad_sequences\nvocab_size = len(vocab) + 1\nword_idx = dict((c, i + 1) for i, c in enumerate(vocab))\nstory_maxlen = max(map(len, (x for x, _, _ in train + test)))\nquery_maxlen = max(map(len, (x for _, x, _ in train + test)))\n\nx, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\ntx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n\nprint('vocab = {}'.format(vocab))\nprint('x.shape = {}'.format(x.shape))\nprint('xq.shape = {}'.format(xq.shape))\nprint('y.shape = {}'.format(y.shape))\nprint('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9bae442f50467fbc6d2aed8597f0f6d4178f9c77"
      },
      "cell_type": "code",
      "source": "print(\"Building the embedding matrix...\")\nGLOVE_PATH = \"../input/glove.6B.50d/\"\n\n\nf = open('../input/glove6b50d/glove.6B.50d.txt', 'r')\nembeddings_index = {}\nfor line in f:\n    values = line.split(\" \")\n    word = values[0]\n    try:\n        coefs = np.asarray(values[1:], dtype='float32')\n    except ValueError:\n        print(values[1:])\n    embeddings_index[word] = coefs\nf.close()\n\nembedding_matrix = np.zeros((len(word_idx) + 1, EMBED_HIDDEN_SIZE))\nfor word, i in word_idx.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\nprint('Found %s word vectors.' % len(embeddings_index))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f0c7e9ef0cfe1b7272dbf3198d75120430cdab3f"
      },
      "cell_type": "code",
      "source": "print('Build model...')\n\nquestion_Input = layers.Input(shape=xq[0].shape, name='question_Input')\nstory_Input = layers.Input(shape=x[0].shape, name='story_Input')\n\n#Embed question\nq_Embedding = layers.Embedding(input_dim = vocab_size, output_dim = EMBED_HIDDEN_SIZE, weights = [embedding_matrix], input_length = story_maxlen)(question_Input)\nq_Encode = recurrent.GRU(EMBED_HIDDEN_SIZE, return_sequences=True, dropout=0.3)(q_Embedding)\n\n#Embed story\ns_Embedding = layers.Embedding(input_dim = vocab_size, output_dim = EMBED_HIDDEN_SIZE, weights = [embedding_matrix], input_length = story_maxlen)(story_Input)\ns_Encode = recurrent.GRU(EMBED_HIDDEN_SIZE, return_sequences=True, dropout=0.3)(s_Embedding)\n\n#Attention Layer\ndot_merge = layers.Dot(axes=[1, 1])([q_Encode, s_Encode])\nflat = Flatten()(dot_merge)\ndense = layers.Dense(EMBED_HIDDEN_SIZE * story_maxlen, activation=\"sigmoid\")(flat)\nreshape = layers.Reshape((story_maxlen, EMBED_HIDDEN_SIZE))(dense)\n\n#Final layers\nsum_merge = layers.Concatenate()([q_Encode, reshape])\n#sum_merge = layers.Add()([q_Encode, s_Encode])\nflat2 = Flatten()(sum_merge)\ndense3 = layers.Dense(EMBED_HIDDEN_SIZE, activation = \"relu\")(flat2)\ndense2 = layers.Dense(vocab_size, activation = \"softmax\")(dense3)\n\nmodel = Model(inputs=[question_Input, story_Input], outputs = [dense2])\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n# checkpoint\n#filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n#callbacks_list = [checkpoint]\n\nprint('Model compiled.')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e11275da47895136f42abb22fe43742f548fc03"
      },
      "cell_type": "code",
      "source": "print('Training')\nprint(model.summary())\nmodel.fit([x, xq], y,\n          batch_size=BATCH_SIZE,\n          epochs=EPOCHS,\n          validation_split=0.1)\nloss, acc = model.evaluate([tx, txq], ty,\n                           batch_size=BATCH_SIZE)\nprint('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d7cad768160445525ca6a63cfa26b5f75d4c260"
      },
      "cell_type": "code",
      "source": "y_pred = model.predict([tx, txq],\n                           batch_size=BATCH_SIZE)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35d4a3b48e11af88aa942c0c36f951a77cd9e74f"
      },
      "cell_type": "code",
      "source": "y_pred_tar = y_pred\nfor i in range(len(y_pred_tar)):\n    y_pred_tar[i][y_pred_tar[i] >= max(y_pred_tar[i])] = 1\n    y_pred_tar[i][y_pred_tar[i] < max(y_pred_tar[i])] = 0\nprint(y_pred_tar[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42d446a1071c757f84e5c91e9d283f05b95df64b"
      },
      "cell_type": "code",
      "source": "total = ty.shape[0]\nEM = 0\nfor i, j in zip(ty, y_pred_tar):\n    if np.argmax(i) == np.argmax(j):\n        EM += 1\nprint(str(EM) + '/' + str(total))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55b1592faa22ba85062a3f3d331c215609164335"
      },
      "cell_type": "code",
      "source": "print(tx)\narticle = ''\nquestion = ''\nprint(word_idx)\ninv_word_idx = {v: k for k, v in word_idx.items()}\nprint(inv_word_idx)\nfor word in tx[6]:\n    if word != 0:\n        article += inv_word_idx[word] + ' '\nfor word in txq[6]:\n    if word != 0:\n        question += inv_word_idx[word] + ' '\nprint(article)\nprint(question)\nprint(inv_word_idx[np.argmax(ty[6])])\nprint(inv_word_idx[np.argmax(y_pred_tar[6])])\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}