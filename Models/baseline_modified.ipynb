{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import os as os\n",
    "import json\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers, callbacks, models\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from datetime import datetime\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 50, 100, 100\n"
     ]
    }
   ],
   "source": [
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 40\n",
    "MODEL_NAME = \"Baseline\"\n",
    "# Regularization parameter\n",
    "LAMBDA = 0.01\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
    "                           'babi_tasks_1-20_v1-2.tar.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab = ['.', '0:', '1:', '2:', '3:', '4:', '5:', '6:', '7:', '8:', '9:', '?', 'Daniel', 'John', 'Mary', 'Sandra', 'Where', 'back', 'bathroom', 'bedroom', 'garden', 'hallway', 'is', 'journeyed', 'kitchen', 'moved', 'office', 'the', 'to', 'travelled', 'went']\n",
      "x.shape = (10000, 78)\n",
      "xq.shape = (10000, 4)\n",
      "y.shape = (10000, 32)\n",
      "story_maxlen, query_maxlen = 78, 4\n"
     ]
    }
   ],
   "source": [
    "challenge = []\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa7_counting_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa8_lists-sets_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa9_simple-negation_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa10_indefinite-knowledge_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa11_basic-coreference_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa12_conjunction-fact_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa13_compound-coreference_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa14_time-reasoning_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa15_basic-deduction_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa16_basic-induction_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa17_positional-reasoning_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa18_size-reasoning_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa19_path-finding_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa20_agents-motivations_{}.txt')\n",
    "\n",
    "\n",
    "def extract_stories(text):\n",
    "    story_out = []\n",
    "    new_story = []\n",
    "    for line in text.readlines():\n",
    "        line = line.decode('utf-8').strip()\n",
    "        number, line = line.split(' ', 1)\n",
    "        if int(number) == 1: \n",
    "            new_story = []\n",
    "        if '\\t' in line:\n",
    "            question, answer, _ = line.split('\\t')\n",
    "            question = re.findall(r\"[\\w']+|[.,!?]\", question)\n",
    "            passage = []\n",
    "            for i,j in enumerate(new_story):\n",
    "                if j:\n",
    "                    passage.append([str(i)+\":\"]+j)\n",
    "            story_out.append((passage, question, answer))\n",
    "        else: \n",
    "            new_story.append(re.findall(r\"[\\w']+|[.,!?]\", line))\n",
    "    \n",
    "    flatten = lambda story_out: reduce(lambda x, y: x + y, story_out)\n",
    "    story_out = [(flatten(p), q, a) for p, q, a in story_out\n",
    "            if not None or len(flatten(story)) < None]\n",
    "    return story_out\n",
    "\n",
    "with tarfile.open(path) as tar:\n",
    "    train = extract_stories(tar.extractfile(challenge[0].format('train')))\n",
    "    test = extract_stories(tar.extractfile(challenge[0].format('test')))\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(test)\n",
    "vocab = set()\n",
    "for story, q, answer in train + test:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "\n",
    "\n",
    "def sort_corpus(data, word_idx):\n",
    "    passage_vect = []\n",
    "    question_vect = []\n",
    "    answer_vect = []\n",
    "    for corpus in data:\n",
    "        passage = corpus[0]\n",
    "        question = corpus[1]\n",
    "        answer = corpus[2]\n",
    "        \n",
    "        passage_num = []\n",
    "        for lines in passage:\n",
    "            passage_num.append(word_idx[lines])\n",
    "        passage_vect.append(passage_num)\n",
    "        question_num = []\n",
    "        for words in question:\n",
    "            question_num.append(word_idx[words])\n",
    "        question_vect.append(question_num)\n",
    "        answer_num = np.zeros(len(word_idx)+1)\n",
    "        answer_num[word_idx[answer]] = 1\n",
    "        answer_vect.append(answer_num)\n",
    "    return(passage_vect, question_vect, answer_vect)\n",
    "\n",
    "\n",
    "passage, question, answer = sort_corpus(train, word_idx)\n",
    "\n",
    "x = pad_sequences(passage, maxlen=story_maxlen)\n",
    "xq = pad_sequences(question, maxlen=query_maxlen)\n",
    "y = np.array(answer)\n",
    "\n",
    "\n",
    "passage, question, answer = sort_corpus(test, word_idx)\n",
    "\n",
    "tx = pad_sequences(passage, maxlen=story_maxlen)\n",
    "txq = pad_sequences(question, maxlen=query_maxlen)\n",
    "ty = np.array(answer)\n",
    "\n",
    "print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xq.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the embedding matrix...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print(\"Building the embedding matrix...\")\n",
    "GLOVE_PATH = '../../glove.6B'\n",
    "\n",
    "#MAC\n",
    "f = open(os.path.join(GLOVE_PATH,\"glove.6B.{}d.txt\".format(EMBED_HIDDEN_SIZE)), 'r', encoding = \"ISO-8859-1\")\n",
    "#WINDOWS\n",
    "#f = open(os.path.join(GLOVE_PATH,\"glove.6B.{}d.txt\".format(EMBED_HIDDEN_SIZE)), 'r', encoding = \"ANSI\")\n",
    "embeddings_index = {}\n",
    "for line in f:\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "    except ValueError:\n",
    "        print(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_idx) + 1, EMBED_HIDDEN_SIZE))\n",
    "for word, i in word_idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "story_Input (InputLayer)        (None, 78)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_Input (InputLayer)     (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 78, 50)       1600        story_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 4, 50)        1600        question_Input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 78, 100)      30300       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 4, 100)       30300       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 78, 100)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 4, 100)       0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 78, 4)        0           reshape_2[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 78, 4)        0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 78, 4)        0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 78, 100)      0           reshape_3[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 78, 200)      0           reshape_2[0][0]                  \n",
      "                                                                 dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 15600)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           780050      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           1632        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 845,482\n",
      "Trainable params: 845,482\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "\n",
    "\n",
    "question_Input = layers.Input(shape=xq[0].shape, name='question_Input')\n",
    "story_Input = layers.Input(shape=x[0].shape, name='story_Input')\n",
    "\n",
    "#Embed question\n",
    "q_Embedding = layers.Embedding(input_dim = vocab_size, output_dim = EMBED_HIDDEN_SIZE, \\\n",
    "                               weights = [embedding_matrix], input_length = query_maxlen)(question_Input)\n",
    "# Bidirectional GRU (optimal dropout approx 0.4 without regularization)\n",
    "q_Encode = layers.Bidirectional(recurrent.GRU(EMBED_HIDDEN_SIZE, return_sequences=True,\\\n",
    "                                              kernel_regularizer = regularizers.l2(LAMBDA), dropout=0.3))(q_Embedding)\n",
    "q_Encode = layers.Reshape((query_maxlen, 2*EMBED_HIDDEN_SIZE))(q_Encode)\n",
    "\n",
    "#Embed story\n",
    "s_Embedding = layers.Embedding(input_dim = vocab_size, output_dim = EMBED_HIDDEN_SIZE, \\\n",
    "                               weights = [embedding_matrix], input_length = story_maxlen)(story_Input)\n",
    "# Bidirectional GRU (optimal dropout approx 0.4 without regularization)\n",
    "s_Encode = layers.Bidirectional(recurrent.GRU(EMBED_HIDDEN_SIZE, return_sequences=True, \\\n",
    "                                              kernel_regularizer = regularizers.l2(LAMBDA), dropout=0.3))(s_Embedding)\n",
    "s_Encode = layers.Reshape((story_maxlen, 2*EMBED_HIDDEN_SIZE))(s_Encode)\n",
    "\n",
    "# Attention Layer\n",
    "# Multiply between context and query to form attention\n",
    "# Resultant matrix should be MxN, taking in Mxd and Nxd \n",
    "# embedded question/answer matrices where d is 2*EMBED_HIDDEN_SIZE\n",
    "dot_merge = layers.Dot(axes = [2,2])([s_Encode, q_Encode])\n",
    "\n",
    "# Flatten and compute softmax for each attent distro\n",
    "flat = Flatten()(dot_merge)\n",
    "dense = layers.Dense(query_maxlen * story_maxlen, kernel_regularizer = regularizers.l2(LAMBDA))(flat)\n",
    "# act = layers.Activation(\"softmax\")(dense)\n",
    "act = layers.Activation(\"softmax\")(dot_merge)\n",
    "\n",
    "\n",
    "# Reshape back into the original dimensions (MxN)\n",
    "act_resh = layers.Reshape((story_maxlen, query_maxlen), input_shape=(query_maxlen,))(act)\n",
    "# Compute attention output as an element-wise multiplication\n",
    "attn_out = layers.Dot(axes=[2,1])([act_resh, q_Encode])\n",
    "# Next we concatenate to form a blended representation of the same dimension as an encoded question,\n",
    "# of which there exists one for every given context hidden state. Should be 4H x 2N\n",
    "blended = layers.Concatenate(axis=2)([s_Encode, attn_out])\n",
    "flat2 = Flatten()(blended)\n",
    "#relu = layers.Activation(\"relu\")(flat2)\n",
    "\n",
    "#####\n",
    "# TODO: Finish Logit + fully connected layer for RELU \n",
    "relu = layers.Dense(EMBED_HIDDEN_SIZE, activation = \"relu\")(flat2)\n",
    "#logit = layers.Dense(1)(relu)\n",
    "#####\n",
    "#print(relu.get_shape(), logit.get_shape())\n",
    "\n",
    "\n",
    "dense2 = layers.Dense(vocab_size, activation = \"softmax\", kernel_regularizer = regularizers.l2(LAMBDA))(relu)\n",
    "######\n",
    "# TODO: Add vanilla softmax at output (no weight vector here, i.e. no Dense) \n",
    "# dense2 = layers.Activation(\"softmax\")(logit)\n",
    "#####\n",
    "\n",
    "model = Model(inputs=[story_Input, question_Input], outputs = [dense2])\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "story_Input (InputLayer)        (None, 78)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_Input (InputLayer)     (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 78, 50)       1600        story_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 4, 50)        1600        question_Input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 78, 100)      30300       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 4, 100)       30300       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 78, 100)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 4, 100)       0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 78, 4)        0           reshape_2[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 78, 4)        0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 78, 4)        0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 78, 100)      0           reshape_3[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 78, 200)      0           reshape_2[0][0]                  \n",
      "                                                                 dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 15600)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           780050      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           1632        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 845,482\n",
      "Trainable params: 845,482\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 9500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "9500/9500 [==============================] - 31s 3ms/step - loss: 3.8009 - acc: 0.2901 - val_loss: 2.4510 - val_acc: 0.5160\n",
      "\n",
      "Epoch 00001: saving model to ../Outputs/Baseline_12-11-2018.h5\n",
      "Epoch 2/100\n",
      "6272/9500 [==================>...........] - ETA: 7s - loss: 2.1512 - acc: 0.4836"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7d119e0a3c40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m           callbacks = [saverCallback])\n\u001b[0m\u001b[1;32m     17\u001b[0m loss, acc = model.evaluate([tx, txq], ty,\n\u001b[1;32m     18\u001b[0m                            batch_size=BATCH_SIZE)\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training')\n",
    "print(model.summary())\n",
    "now = datetime.now()\n",
    "currDate = \"{}-{}-{}\".format(now.month, now.day, now.year)\n",
    "\n",
    "outpath = \"../Outputs\"\n",
    "modelFilename = \"{}_{}.h5\".format(MODEL_NAME, currDate)\n",
    "histFilename = \"{}_history_{}.json\".format(MODEL_NAME, currDate)\n",
    "\n",
    "\n",
    "saverCallback = callbacks.ModelCheckpoint(filepath = os.path.join(outpath, modelFilename), monitor = \"val_loss\", verbose = 1)\n",
    "history = model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=100,\n",
    "          validation_split=0.05,\n",
    "          callbacks = [saverCallback])\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "\n",
    "with open(os.path.join(outpath, histFilename), 'w') as histFile:\n",
    "    json.dump(history.history, histFile)\n",
    "    \n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = \"../Outputs\"\n",
    "hist = json.load(open(os.path.join(outpath, \"Baseline_history_11-17-2018.json\"), \"r\"))\n",
    "\n",
    "xlen = len(hist[list(hist.keys())[0]])\n",
    "print(xlen)\n",
    "x_pts = [i for i in range(xlen)]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey = False)\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(15)\n",
    "sns.lineplot(x=x_pts, y=hist[\"loss\"], ax = ax1).set_title(\"loss\")\n",
    "sns.lineplot(x=x_pts, y=hist[\"acc\"], ax = ax2).set_title(\"acc\")\n",
    "fig.savefig(os.path.join(outpath, \"{}_train_metrics.png\".format(MODEL_NAME)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey = False)\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(15)\n",
    "sns.lineplot(x=x_pts, y=hist[\"val_loss\"], ax = ax1).set_title(\"val_loss\")\n",
    "sns.lineplot(x=x_pts, y=hist[\"val_acc\"], ax = ax2).set_title(\"val_acc\")\n",
    "fig.savefig(os.path.join(outpath, \"{}_val_metrics.png\".format(MODEL_NAME)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
