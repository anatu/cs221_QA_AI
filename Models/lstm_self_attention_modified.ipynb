{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import os as os\n",
    "import json\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers, callbacks, models\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from datetime import datetime\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 40\n",
    "MODEL_NAME = \"LSTM_SelfAttention\"\n",
    "# Regularization parameter\n",
    "LAMBDA = 0.02\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
    "                           'babi_tasks_1-20_v1-2.tar.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = []\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa7_counting_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa8_lists-sets_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa9_simple-negation_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa10_indefinite-knowledge_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa11_basic-coreference_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa12_conjunction-fact_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa13_compound-coreference_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa14_time-reasoning_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa15_basic-deduction_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa16_basic-induction_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa17_positional-reasoning_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa18_size-reasoning_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa19_path-finding_{}.txt')\n",
    "challenge.append('tasks_1-20_v1-2/en-10k/qa20_agents-motivations_{}.txt')\n",
    "\n",
    "\n",
    "def extract_stories(text):\n",
    "    story_out = []\n",
    "    new_story = []\n",
    "    for line in text.readlines():\n",
    "        line = line.decode('utf-8').strip()\n",
    "        number, line = line.split(' ', 1)\n",
    "        if int(number) == 1: \n",
    "            new_story = []\n",
    "        if '\\t' in line:\n",
    "            question, answer, _ = line.split('\\t')\n",
    "            question = re.findall(r\"[\\w']+|[.,!?]\", question)\n",
    "            passage = []\n",
    "            for i,j in enumerate(new_story):\n",
    "                if j:\n",
    "                    passage.append([str(i)+\":\"]+j)\n",
    "            story_out.append((passage, question, answer))\n",
    "        else: \n",
    "            new_story.append(re.findall(r\"[\\w']+|[.,!?]\", line))\n",
    "    \n",
    "    flatten = lambda story_out: reduce(lambda x, y: x + y, story_out)\n",
    "    story_out = [(flatten(p), q, a) for p, q, a in story_out\n",
    "            if not None or len(flatten(story)) < None]\n",
    "    return story_out\n",
    "\n",
    "with tarfile.open(path) as tar:\n",
    "    train = extract_stories(tar.extractfile(challenge[0].format('train')))\n",
    "    test = extract_stories(tar.extractfile(challenge[0].format('test')))\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(test)\n",
    "vocab = set()\n",
    "for story, q, answer in train + test:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "\n",
    "def sort_corpus(data, word_idx):\n",
    "    passage_vect = []\n",
    "    question_vect = []\n",
    "    answer_vect = []\n",
    "    for corpus in data:\n",
    "        passage = corpus[0]\n",
    "        question = corpus[1]\n",
    "        answer = corpus[2]\n",
    "        \n",
    "        passage_num = []\n",
    "        for lines in passage:\n",
    "            passage_num.append(word_idx[lines])\n",
    "        passage_vect.append(passage_num)\n",
    "        question_num = []\n",
    "        for words in question:\n",
    "            question_num.append(word_idx[words])\n",
    "        question_vect.append(question_num)\n",
    "        answer_num = np.zeros(len(word_idx)+1)\n",
    "        answer_num[word_idx[answer]] = 1\n",
    "        answer_vect.append(answer_num)\n",
    "    return(passage_vect, question_vect, answer_vect)\n",
    "\n",
    "\n",
    "passage, question, answer = sort_corpus(train, word_idx)\n",
    "\n",
    "x = pad_sequences(passage, maxlen=story_maxlen)\n",
    "xq = pad_sequences(question, maxlen=query_maxlen)\n",
    "y = np.array(answer)\n",
    "\n",
    "\n",
    "passage, question, answer = sort_corpus(test, word_idx)\n",
    "\n",
    "tx = pad_sequences(passage, maxlen=story_maxlen)\n",
    "txq = pad_sequences(question, maxlen=query_maxlen)\n",
    "ty = np.array(answer)\n",
    "\n",
    "print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xq.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building the embedding matrix...\")\n",
    "GLOVE_PATH = '../../glove.6B'\n",
    "\n",
    "\n",
    "#MAC\n",
    "f = open(os.path.join(GLOVE_PATH,\"glove.6B.{}d.txt\".format(EMBED_HIDDEN_SIZE)), 'r', encoding = \"ISO-8859-1\")\n",
    "#WINDOWS\n",
    "#f = open(os.path.join(GLOVE_PATH,\"glove.6B.{}d.txt\".format(EMBED_HIDDEN_SIZE)), 'r', encoding = \"ANSI\")\n",
    "embeddings_index = {}\n",
    "for line in f:\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "    except ValueError:\n",
    "        print(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_idx) + 1, EMBED_HIDDEN_SIZE))\n",
    "for word, i in word_idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "\n",
    "\n",
    "\n",
    "question_Input = layers.Input(shape=xq[0].shape, name='question_Input')\n",
    "story_Input = layers.Input(shape=x[0].shape, name='story_Input')\n",
    "\n",
    "# Embed question\n",
    "q_Embedding = layers.Embedding(input_dim = vocab_size, output_dim = EMBED_HIDDEN_SIZE, \\\n",
    "                               weights = [embedding_matrix], input_length = query_maxlen, trainable=False)(question_Input)\n",
    "# Bidirectional GRU (optimal dropout approx 0.4 without regularization)\n",
    "q_Encode = layers.Bidirectional(recurrent.LSTM(EMBED_HIDDEN_SIZE, return_sequences=True,\\\n",
    "                                              kernel_regularizer = regularizers.l2(LAMBDA), dropout=0.3))(q_Embedding)\n",
    "q_Encode = layers.Reshape((query_maxlen, 2*EMBED_HIDDEN_SIZE))(q_Encode)\n",
    "\n",
    "# Embed story\n",
    "s_Embedding = layers.Embedding(input_dim = vocab_size, output_dim = EMBED_HIDDEN_SIZE, \\\n",
    "                               weights = [embedding_matrix], input_length = story_maxlen, trainable=False)(story_Input)\n",
    "# Bidirectional GRU (optimal dropout approx 0.4 without regularization)\n",
    "s_Encode = layers.Bidirectional(recurrent.LSTM(EMBED_HIDDEN_SIZE, return_sequences=True, \\\n",
    "                                              kernel_regularizer = regularizers.l2(LAMBDA), dropout=0.3))(s_Embedding)\n",
    "s_Encode = layers.Reshape((story_maxlen, 2*EMBED_HIDDEN_SIZE))(s_Encode)\n",
    "\n",
    "# Regular Attention Layer\n",
    "# Multiply between context and query to form attention\n",
    "# Resultant matrix should be MxN, taking in Mxd and Nxd \n",
    "# embedded question/answer matrices where d is 2*EMBED_HIDDEN_SIZE\n",
    "dot_merge = layers.Dot(axes = [2,2])([s_Encode, q_Encode])\n",
    "\n",
    "# Self-Attention Layer\n",
    "self_attention = SeqSelfAttention(attention_activation = \"softmax\", \n",
    "                      kernel_regularizer=regularizers.l2(LAMBDA),\n",
    "                       attention_regularizer_weight=LAMBDA)(dot_merge)\n",
    "\n",
    "# Flatten and compute softmax for each attent distro\n",
    "flat = Flatten()(self_attention)\n",
    "dense = layers.Dense(query_maxlen * story_maxlen, kernel_regularizer = regularizers.l2(LAMBDA))(flat)\n",
    "dense_d = layers.Dropout(rate = 0.3)(dense)\n",
    "#act = layers.Activation(\"softmax\")(dense_d)\n",
    "act = layers.Activation(\"softmax\")(dot_merge)\n",
    "\n",
    "\n",
    "\n",
    "# Reshape back into the original dimensions (MxN)\n",
    "act_resh = layers.Reshape((story_maxlen, query_maxlen), input_shape=(query_maxlen,))(act)\n",
    "# Compute attention output as an element-wise multiplication\n",
    "attn_out = layers.Dot(axes=[2,1])([act_resh, q_Encode])\n",
    "# Next we concatenate to form a blended representation of the same dimension as an encoded question,\n",
    "# of which there exists one for every given context hidden state. Should be 4H x 2N\n",
    "blended = layers.Concatenate(axis=2)([s_Encode, attn_out])\n",
    "flat2 = Flatten()(blended)\n",
    "#relu = layers.Activation(\"relu\")(flat2)\n",
    "\n",
    "relu = layers.Dense(EMBED_HIDDEN_SIZE, activation = \"relu\", kernel_regularizer = regularizers.l2(LAMBDA))(flat2)\n",
    "relu_d = layers.Dropout(rate = 0.3)(relu)\n",
    "\n",
    "dense2 = layers.Dense(vocab_size, activation = \"softmax\")(relu_d)\n",
    "\n",
    "model = Model(inputs=[story_Input, question_Input], outputs = [dense2])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training')\n",
    "print(model.summary())\n",
    "now = datetime.now()\n",
    "currDate = \"{}-{}-{}\".format(now.month, now.day, now.year)\n",
    "\n",
    "outpath = \"../Outputs\"\n",
    "modelFilename = \"{}_{}.h5\".format(MODEL_NAME, currDate)\n",
    "histFilename = \"{}_history_{}.json\".format(MODEL_NAME, currDate)\n",
    "\n",
    "\n",
    "saverCallback = callbacks.ModelCheckpoint(filepath = os.path.join(outpath, modelFilename), monitor = \"val_loss\", verbose = 1)\n",
    "history = model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=100,\n",
    "          validation_split=0.05,\n",
    "          callbacks = [saverCallback])\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "\n",
    "with open(os.path.join(outpath, histFilename), 'w') as histFile:\n",
    "    json.dump(history.history, histFile)\n",
    "    \n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = \"../Outputs\"\n",
    "hist = json.load(open(os.path.join(outpath, \"LSTM_SelfAttention_history_11-19-2018.json\"), \"r\"))\n",
    "\n",
    "xlen = len(hist[list(hist.keys())[0]])\n",
    "print(xlen)\n",
    "x_pts = [i for i in range(xlen)]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey = False)\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(15)\n",
    "sns.lineplot(x=x_pts, y=hist[\"loss\"], ax = ax1).set_title(\"loss\")\n",
    "sns.lineplot(x=x_pts, y=hist[\"acc\"], ax = ax2).set_title(\"acc\")\n",
    "fig.savefig(os.path.join(outpath, \"{}_train_metrics.png\".format(MODEL_NAME)))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey = False)\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(15)\n",
    "sns.lineplot(x=x_pts, y=hist[\"val_loss\"], ax = ax1).set_title(\"val_loss\")\n",
    "sns.lineplot(x=x_pts, y=hist[\"val_acc\"], ax = ax2).set_title(\"val_acc\")\n",
    "fig.savefig(os.path.join(outpath, \"{}_val_metrics.png\".format(MODEL_NAME)))\n",
    "\n",
    "\n",
    "metrics = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
