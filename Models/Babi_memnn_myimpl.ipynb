{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import os as os\n",
    "import json\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers, callbacks, models\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from datetime import datetime\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        if int(nid) == 1: story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            substory = [[str(i)+\":\"]+x for i,x in enumerate(story) if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else: story.append(tokenize(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f):\n",
    "    data = parse_stories(f.readlines())\n",
    "    return [(story, q, answer) for story, q, answer in data]\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    X = []; Xq = []; Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [[word_idx[w] for w in s] for s in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        y = [word_idx[answer]]\n",
    "        X.append(x); Xq.append(xq); Y.append(y)\n",
    "    return ([pad_sequences(x, maxlen=story_maxlen) for x in X],\n",
    "            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n",
    "\n",
    "def do_flatten(el): \n",
    "    return isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes))\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if do_flatten(el): yield from flatten(el)\n",
    "        else: yield el\n",
    "            \n",
    "def stack_inputs(inputs):\n",
    "    for i,it in enumerate(inputs):\n",
    "        inputs[i] = np.concatenate([it, \n",
    "                           np.zeros((story_maxsents-it.shape[0],story_maxlen), 'int')])\n",
    "    return np.stack(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 50, 100, 100\n"
     ]
    }
   ],
   "source": [
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 40\n",
    "MODEL_NAME = \"LSTM_SelfAttention\"\n",
    "# Regularization parameter\n",
    "LAMBDA = 0.02\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
    "                           'babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n",
    "          '.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nlandy\\Anaconda3\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "# Default QA1 with 1000 samples\n",
    "#challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
    "# QA1 with 10,000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n",
    "# QA2 with 1000 samples\n",
    "#challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt'\n",
    "# QA2 with 10,000 samples\n",
    "challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'\n",
    "with tarfile.open(path) as tar:\n",
    "    train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "    test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "np.random.shuffle(train_stories)\n",
    "np.random.shuffle(test_stories)\n",
    "\n",
    "stories = train_stories + test_stories\n",
    "\n",
    "story_maxlen = max((len(s) for x, _, _ in stories for s in x))\n",
    "story_maxsents = max((len(x) for x, _, _ in stories))\n",
    "query_maxlen = max(len(x) for _, x, _ in stories)\n",
    "\n",
    "vocab = sorted(set(flatten(stories)))\n",
    "vocab.insert(0, '<PAD>')\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_idx = dict((c, i) for i, c in enumerate(vocab))\n",
    "\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories, \n",
    "     word_idx, story_maxlen, query_maxlen)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories, \n",
    "     word_idx, story_maxlen, query_maxlen)\n",
    "\n",
    "inputs_train = stack_inputs(inputs_train)\n",
    "inputs_test = stack_inputs(inputs_test)\n",
    "\n",
    "inps = [inputs_train, queries_train]\n",
    "val_inps = [inputs_test, queries_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nlandy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.1363 - acc: 0.1690 - val_loss: 2.0746 - val_acc: 0.1460\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.8185 - acc: 0.1830 - val_loss: 1.9328 - val_acc: 0.1870\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 713us/step - loss: 1.8089 - acc: 0.2040 - val_loss: 1.8334 - val_acc: 0.1760\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 1.8071 - acc: 0.1870 - val_loss: 1.8663 - val_acc: 0.1870\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 651us/step - loss: 1.7812 - acc: 0.2220 - val_loss: 1.8387 - val_acc: 0.1830\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 1.6783 - acc: 0.3270 - val_loss: 1.8265 - val_acc: 0.2240\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 807us/step - loss: 1.5635 - acc: 0.3900 - val_loss: 1.7593 - val_acc: 0.2770\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 802us/step - loss: 1.4783 - acc: 0.4400 - val_loss: 1.7188 - val_acc: 0.2930\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 749us/step - loss: 1.4122 - acc: 0.4780 - val_loss: 1.7701 - val_acc: 0.2910\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 798us/step - loss: 1.3608 - acc: 0.4870 - val_loss: 1.8604 - val_acc: 0.3020\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 724us/step - loss: 1.3328 - acc: 0.5020 - val_loss: 1.9006 - val_acc: 0.2980\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 701us/step - loss: 1.2918 - acc: 0.5080 - val_loss: 1.9618 - val_acc: 0.3030\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 722us/step - loss: 1.2554 - acc: 0.5120 - val_loss: 1.9246 - val_acc: 0.2980\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 769us/step - loss: 1.2386 - acc: 0.5300 - val_loss: 2.0413 - val_acc: 0.3050\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 728us/step - loss: 1.2270 - acc: 0.5390 - val_loss: 1.9240 - val_acc: 0.2880\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.2010 - acc: 0.5430 - val_loss: 2.0020 - val_acc: 0.2960\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 817us/step - loss: 1.1890 - acc: 0.5630 - val_loss: 1.9575 - val_acc: 0.2970\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 700us/step - loss: 1.1720 - acc: 0.5610 - val_loss: 2.2027 - val_acc: 0.2900\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 731us/step - loss: 1.1712 - acc: 0.5490 - val_loss: 2.0787 - val_acc: 0.2920\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 668us/step - loss: 1.1483 - acc: 0.5850 - val_loss: 2.0294 - val_acc: 0.2930\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 831us/step - loss: 1.1527 - acc: 0.5730 - val_loss: 2.1356 - val_acc: 0.2790\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.1478 - acc: 0.5690 - val_loss: 2.1499 - val_acc: 0.2840\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.1240 - acc: 0.5800 - val_loss: 2.2058 - val_acc: 0.2800\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.1329 - acc: 0.5740 - val_loss: 2.2794 - val_acc: 0.2760\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 773us/step - loss: 1.1106 - acc: 0.5850 - val_loss: 2.2281 - val_acc: 0.2880\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 756us/step - loss: 1.1070 - acc: 0.5960 - val_loss: 2.1835 - val_acc: 0.2710\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 702us/step - loss: 1.1119 - acc: 0.5820 - val_loss: 2.3765 - val_acc: 0.2520\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 802us/step - loss: 1.1095 - acc: 0.5920 - val_loss: 2.3119 - val_acc: 0.2680\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 835us/step - loss: 1.1123 - acc: 0.6040 - val_loss: 2.3332 - val_acc: 0.2700\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 701us/step - loss: 1.0859 - acc: 0.5970 - val_loss: 2.2519 - val_acc: 0.2540\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 824us/step - loss: 1.0883 - acc: 0.6020 - val_loss: 2.4633 - val_acc: 0.2730\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 767us/step - loss: 1.0858 - acc: 0.5970 - val_loss: 2.4512 - val_acc: 0.2670\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 1.0871 - acc: 0.6060 - val_loss: 2.4116 - val_acc: 0.2690\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 749us/step - loss: 1.0727 - acc: 0.6190 - val_loss: 2.4485 - val_acc: 0.2610\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 785us/step - loss: 1.0632 - acc: 0.6190 - val_loss: 2.6496 - val_acc: 0.2750\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.0637 - acc: 0.6300 - val_loss: 2.5830 - val_acc: 0.2380\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 715us/step - loss: 1.0778 - acc: 0.6130 - val_loss: 2.6426 - val_acc: 0.2410\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 779us/step - loss: 1.0707 - acc: 0.6120 - val_loss: 2.5353 - val_acc: 0.2540\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 720us/step - loss: 1.0661 - acc: 0.6190 - val_loss: 2.5214 - val_acc: 0.2440\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 798us/step - loss: 1.0631 - acc: 0.6050 - val_loss: 2.6316 - val_acc: 0.2660\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 831us/step - loss: 1.0467 - acc: 0.6080 - val_loss: 2.6631 - val_acc: 0.2550\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 738us/step - loss: 1.0529 - acc: 0.6190 - val_loss: 2.6164 - val_acc: 0.2640\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 725us/step - loss: 1.0529 - acc: 0.6160 - val_loss: 2.5184 - val_acc: 0.2610\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 735us/step - loss: 1.0533 - acc: 0.6200 - val_loss: 2.5606 - val_acc: 0.2620\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 834us/step - loss: 1.0397 - acc: 0.6170 - val_loss: 2.4890 - val_acc: 0.2790\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 724us/step - loss: 1.0561 - acc: 0.6140 - val_loss: 2.5186 - val_acc: 0.2520\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 806us/step - loss: 1.0414 - acc: 0.6130 - val_loss: 2.6594 - val_acc: 0.2610\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 742us/step - loss: 1.0462 - acc: 0.6160 - val_loss: 2.7432 - val_acc: 0.2640\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 799us/step - loss: 1.0454 - acc: 0.6080 - val_loss: 2.6746 - val_acc: 0.2610\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 819us/step - loss: 1.0343 - acc: 0.6230 - val_loss: 2.7050 - val_acc: 0.2290\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 785us/step - loss: 1.0485 - acc: 0.6060 - val_loss: 2.6018 - val_acc: 0.2420\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 711us/step - loss: 1.0390 - acc: 0.6150 - val_loss: 2.7790 - val_acc: 0.2500\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 860us/step - loss: 1.0334 - acc: 0.6240 - val_loss: 2.6515 - val_acc: 0.2490\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 736us/step - loss: 1.0336 - acc: 0.6130 - val_loss: 2.7374 - val_acc: 0.2490\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 818us/step - loss: 1.0281 - acc: 0.6310 - val_loss: 2.6884 - val_acc: 0.2510\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 816us/step - loss: 1.0293 - acc: 0.6220 - val_loss: 2.7183 - val_acc: 0.2400\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 783us/step - loss: 1.0296 - acc: 0.6180 - val_loss: 2.8969 - val_acc: 0.2600\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 729us/step - loss: 1.0362 - acc: 0.6270 - val_loss: 3.0295 - val_acc: 0.2430\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 731us/step - loss: 1.0366 - acc: 0.6250 - val_loss: 2.8395 - val_acc: 0.2660\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 1.0119 - acc: 0.6270 - val_loss: 2.8084 - val_acc: 0.2320\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 801us/step - loss: 1.0132 - acc: 0.6340 - val_loss: 3.0344 - val_acc: 0.2530\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.0268 - acc: 0.6260 - val_loss: 2.7643 - val_acc: 0.2470\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 802us/step - loss: 1.0208 - acc: 0.6150 - val_loss: 3.0224 - val_acc: 0.2660\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 747us/step - loss: 1.0111 - acc: 0.6200 - val_loss: 2.9053 - val_acc: 0.2520\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 866us/step - loss: 1.0151 - acc: 0.6090 - val_loss: 2.9445 - val_acc: 0.2500\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 697us/step - loss: 1.0195 - acc: 0.6280 - val_loss: 2.8927 - val_acc: 0.2400\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 1.0050 - acc: 0.6410 - val_loss: 2.8286 - val_acc: 0.2530\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 723us/step - loss: 1.0111 - acc: 0.6320 - val_loss: 2.8649 - val_acc: 0.2460\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.0046 - acc: 0.6280 - val_loss: 2.9485 - val_acc: 0.2360\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.0183 - acc: 0.6270 - val_loss: 2.8961 - val_acc: 0.2360\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 668us/step - loss: 1.0018 - acc: 0.6210 - val_loss: 2.8345 - val_acc: 0.2530\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 765us/step - loss: 1.0131 - acc: 0.6270 - val_loss: 3.0641 - val_acc: 0.2480\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 749us/step - loss: 0.9949 - acc: 0.6220 - val_loss: 2.9312 - val_acc: 0.2600\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 718us/step - loss: 1.0049 - acc: 0.6440 - val_loss: 3.0022 - val_acc: 0.2520\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 749us/step - loss: 1.0091 - acc: 0.6400 - val_loss: 2.9780 - val_acc: 0.2360\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 895us/step - loss: 0.9836 - acc: 0.6380 - val_loss: 3.1098 - val_acc: 0.2580\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 807us/step - loss: 1.0031 - acc: 0.6190 - val_loss: 3.0395 - val_acc: 0.2120\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 878us/step - loss: 0.9813 - acc: 0.6230 - val_loss: 2.9986 - val_acc: 0.2450\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 746us/step - loss: 0.9958 - acc: 0.6260 - val_loss: 3.1149 - val_acc: 0.2400\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 891us/step - loss: 0.9965 - acc: 0.6350 - val_loss: 3.0557 - val_acc: 0.2400\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 955us/step - loss: 0.9859 - acc: 0.6370 - val_loss: 3.0761 - val_acc: 0.2380\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 957us/step - loss: 0.9982 - acc: 0.6290 - val_loss: 3.1609 - val_acc: 0.2320\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 932us/step - loss: 0.9909 - acc: 0.6330 - val_loss: 3.1848 - val_acc: 0.2430\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 947us/step - loss: 0.9864 - acc: 0.6340 - val_loss: 3.0638 - val_acc: 0.2400\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 733us/step - loss: 0.9822 - acc: 0.6290 - val_loss: 3.3504 - val_acc: 0.2530\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 715us/step - loss: 0.9939 - acc: 0.6270 - val_loss: 3.0633 - val_acc: 0.2470\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 817us/step - loss: 0.9829 - acc: 0.6370 - val_loss: 3.1340 - val_acc: 0.2570\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 714us/step - loss: 0.9754 - acc: 0.6430 - val_loss: 3.0977 - val_acc: 0.2270\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 0.9817 - acc: 0.6450 - val_loss: 3.0475 - val_acc: 0.2340\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 820us/step - loss: 0.9836 - acc: 0.6440 - val_loss: 3.2268 - val_acc: 0.2280\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 846us/step - loss: 0.9835 - acc: 0.6460 - val_loss: 3.1098 - val_acc: 0.2440\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 834us/step - loss: 0.9807 - acc: 0.6330 - val_loss: 3.1415 - val_acc: 0.2420\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 722us/step - loss: 0.9832 - acc: 0.6300 - val_loss: 3.0375 - val_acc: 0.2290\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 0.9783 - acc: 0.6400 - val_loss: 3.1155 - val_acc: 0.2130\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 732us/step - loss: 0.9785 - acc: 0.6290 - val_loss: 3.2383 - val_acc: 0.2390\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 783us/step - loss: 0.9695 - acc: 0.6380 - val_loss: 3.2203 - val_acc: 0.2380\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 750us/step - loss: 0.9766 - acc: 0.6460 - val_loss: 3.2025 - val_acc: 0.2330\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 851us/step - loss: 0.9778 - acc: 0.6560 - val_loss: 3.1357 - val_acc: 0.2360\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 705us/step - loss: 0.9676 - acc: 0.6530 - val_loss: 3.3457 - val_acc: 0.2170\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 702us/step - loss: 0.9908 - acc: 0.6470 - val_loss: 3.3222 - val_acc: 0.2360\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 88, 8)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 5, 20)        2480        input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 88, 8, 20)    2480        input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 20)           0           embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 88, 20)       0           time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 1, 20)        0           lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_6 (Dot)                     (None, 88, 1)        0           lambda_16[0][0]                  \n",
      "                                                                 reshape_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 88)           0           dot_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 88)           0           reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 88, 8, 20)    2480        input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 88, 1)        0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 88, 20)       0           time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dot_7 (Dot)                     (None, 1, 20)        0           reshape_16[0][0]                 \n",
      "                                                                 lambda_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 20)           0           dot_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 124)          2604        reshape_17[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 10,044\n",
      "Trainable params: 10,044\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "\n",
    "emb_dim = 20\n",
    "def emb_sent_bow(inp):\n",
    "    emb = layers.TimeDistributed(Embedding(vocab_size, emb_dim))(inp)\n",
    "    return layers.Lambda(lambda x: K.sum(x, 2))(emb)\n",
    "inp_story = layers.Input((story_maxsents, story_maxlen))\n",
    "emb_story = emb_sent_bow(inp_story)\n",
    "inp_story.shape, emb_story.shape\n",
    "inp_q = layers.Input((query_maxlen,))\n",
    "emb_q = layers.Embedding(vocab_size, emb_dim)(inp_q)\n",
    "emb_q = layers.Lambda(lambda x: K.sum(x, 1))(emb_q)\n",
    "emb_q = layers.Reshape((1, emb_dim))(emb_q)\n",
    "inp_q.shape, emb_q.shape\n",
    "x = layers.Dot(axes=2)([emb_story, emb_q])\n",
    "x = layers.Reshape((story_maxsents,))(x)\n",
    "x = layers.Activation('softmax')(x)\n",
    "match = layers.Reshape((story_maxsents,1))(x)\n",
    "match.shape\n",
    "emb_c = emb_sent_bow(inp_story)\n",
    "x = layers.Dot(axes=1)([match, emb_c])\n",
    "response = layers.Reshape((emb_dim,))(x)\n",
    "res = layers.Dense(vocab_size, activation='softmax')(response)\n",
    "model = Model([inp_story, inp_q], res)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "K.set_value(model.optimizer.lr, 1e-2)\n",
    "hist=model.fit(inps, answers_train, nb_epoch=100, batch_size=32,\n",
    "           validation_data=(val_inps, answers_test))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nlandy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 26s 3ms/step - loss: 1.9122 - acc: 0.1723 - val_loss: 1.8170 - val_acc: 0.1600\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 25s 3ms/step - loss: 1.8125 - acc: 0.1735 - val_loss: 1.8120 - val_acc: 0.1670\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 26s 3ms/step - loss: 1.8045 - acc: 0.1679 - val_loss: 1.7999 - val_acc: 0.1650\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 26s 3ms/step - loss: 1.8026 - acc: 0.1681 - val_loss: 1.8054 - val_acc: 0.1600\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 26s 3ms/step - loss: 1.7999 - acc: 0.1664 - val_loss: 1.8011 - val_acc: 0.1600\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.7978 - acc: 0.1750 - val_loss: 1.7979 - val_acc: 0.1870\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.7973 - acc: 0.1717 - val_loss: 1.8022 - val_acc: 0.1870\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.7987 - acc: 0.1802 - val_loss: 1.7998 - val_acc: 0.2220\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 26s 3ms/step - loss: 1.8147 - acc: 0.1821 - val_loss: 1.7976 - val_acc: 0.1600\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.7949 - acc: 0.2253 - val_loss: 1.7680 - val_acc: 0.2810\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.6252 - acc: 0.4852 - val_loss: 1.3475 - val_acc: 0.6480\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.3609 - acc: 0.6306 - val_loss: 1.1041 - val_acc: 0.6940\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 26s 3ms/step - loss: 1.2381 - acc: 0.6567 - val_loss: 1.0823 - val_acc: 0.7120\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 26s 3ms/step - loss: 1.1696 - acc: 0.6694 - val_loss: 1.0174 - val_acc: 0.7310\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.1143 - acc: 0.6739 - val_loss: 1.0098 - val_acc: 0.7180\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0814 - acc: 0.6913 - val_loss: 0.9746 - val_acc: 0.7300\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0894 - acc: 0.6790 - val_loss: 0.9292 - val_acc: 0.7310\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0805 - acc: 0.6817 - val_loss: 0.9000 - val_acc: 0.7270\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0716 - acc: 0.6849 - val_loss: 0.9156 - val_acc: 0.7340\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0603 - acc: 0.6840 - val_loss: 1.0203 - val_acc: 0.7050\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0445 - acc: 0.6903 - val_loss: 0.9802 - val_acc: 0.7080\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 26s 3ms/step - loss: 1.0459 - acc: 0.6886 - val_loss: 0.9589 - val_acc: 0.7270\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0506 - acc: 0.6877 - val_loss: 0.9487 - val_acc: 0.6840\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0396 - acc: 0.6865 - val_loss: 0.9127 - val_acc: 0.7420\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0395 - acc: 0.6916 - val_loss: 0.9178 - val_acc: 0.7240\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0381 - acc: 0.6918 - val_loss: 0.8987 - val_acc: 0.7220\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0333 - acc: 0.6913 - val_loss: 0.8538 - val_acc: 0.7530\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 1.0332 - acc: 0.6895 - val_loss: 0.8978 - val_acc: 0.7390\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0149 - acc: 0.7015 - val_loss: 0.9317 - val_acc: 0.7120\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0386 - acc: 0.6875 - val_loss: 0.9121 - val_acc: 0.7360\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0340 - acc: 0.6930 - val_loss: 0.9306 - val_acc: 0.7110\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0333 - acc: 0.6965 - val_loss: 0.8957 - val_acc: 0.7210\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0213 - acc: 0.6972 - val_loss: 0.9714 - val_acc: 0.7070\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0300 - acc: 0.6931 - val_loss: 0.9057 - val_acc: 0.7270\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0317 - acc: 0.6956 - val_loss: 0.8804 - val_acc: 0.7450\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0282 - acc: 0.6953 - val_loss: 0.8723 - val_acc: 0.7520\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0287 - acc: 0.6913 - val_loss: 0.8935 - val_acc: 0.7510\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0116 - acc: 0.6979 - val_loss: 0.8474 - val_acc: 0.7420\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 1.0316 - acc: 0.6917 - val_loss: 0.9792 - val_acc: 0.6980\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0362 - acc: 0.6971 - val_loss: 0.8967 - val_acc: 0.7500\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0341 - acc: 0.6918 - val_loss: 0.9490 - val_acc: 0.7030\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0114 - acc: 0.6962 - val_loss: 0.9423 - val_acc: 0.7170\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 1.0312 - acc: 0.6942 - val_loss: 0.9139 - val_acc: 0.7230\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0149 - acc: 0.7059 - val_loss: 0.8819 - val_acc: 0.7420\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0168 - acc: 0.7009 - val_loss: 0.8986 - val_acc: 0.7460\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0164 - acc: 0.7055 - val_loss: 0.8526 - val_acc: 0.7630\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 1.0213 - acc: 0.6987 - val_loss: 0.9082 - val_acc: 0.7490\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0180 - acc: 0.6946 - val_loss: 0.9271 - val_acc: 0.7380\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0398 - acc: 0.6910 - val_loss: 0.8762 - val_acc: 0.7420\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0256 - acc: 0.6979 - val_loss: 0.8291 - val_acc: 0.7620\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0272 - acc: 0.6926 - val_loss: 0.8709 - val_acc: 0.7470\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.0016 - acc: 0.7071 - val_loss: 0.9132 - val_acc: 0.7110\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0190 - acc: 0.6929 - val_loss: 0.8756 - val_acc: 0.7350\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0233 - acc: 0.6986 - val_loss: 0.8587 - val_acc: 0.7590\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0205 - acc: 0.6951 - val_loss: 0.8609 - val_acc: 0.7530\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0171 - acc: 0.6983 - val_loss: 0.8588 - val_acc: 0.7380\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0061 - acc: 0.7002 - val_loss: 0.8623 - val_acc: 0.7300\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 1.0151 - acc: 0.7029 - val_loss: 0.8940 - val_acc: 0.7450\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 25s 2ms/step - loss: 1.0284 - acc: 0.6990 - val_loss: 0.8580 - val_acc: 0.7440\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 25s 3ms/step - loss: 1.0179 - acc: 0.6999 - val_loss: 0.8751 - val_acc: 0.7550\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 26s 3ms/step - loss: 1.0197 - acc: 0.6954 - val_loss: 0.8471 - val_acc: 0.7410\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 25s 2ms/step - loss: 1.0263 - acc: 0.6921 - val_loss: 0.8759 - val_acc: 0.7500\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 25s 3ms/step - loss: 1.0281 - acc: 0.6960 - val_loss: 0.9190 - val_acc: 0.7500\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 26s 3ms/step - loss: 1.0245 - acc: 0.6982 - val_loss: 0.8920 - val_acc: 0.7410\n",
      "Epoch 65/100\n",
      " 3104/10000 [========>.....................] - ETA: 17s - loss: 1.0036 - acc: 0.7033"
     ]
    }
   ],
   "source": [
    "emb_dim = 30\n",
    "def emb_sent_bow(inp):\n",
    "    emb_op = layers.TimeDistributed(Embedding(vocab_size, emb_dim, embeddings_regularizer=regularizers.l2(0.002)))\n",
    "    emb = emb_op(inp)\n",
    "    emb = layers.Dropout(0.1)(emb)\n",
    "    emb = layers.Lambda(lambda x: K.sum(x, 2))(emb)\n",
    "#     return Elemwise(0, False)(emb), emb_op\n",
    "    return emb, emb_op\n",
    "inp_story = layers.Input((story_maxsents, story_maxlen))\n",
    "inp_q = layers.Input((query_maxlen,))\n",
    "emb_story, emb_story_op = emb_sent_bow(inp_story)\n",
    "emb_q = emb_story_op.layer(inp_q)\n",
    "emb_q = layers.Lambda(lambda x: K.sum(x, 1))(emb_q)\n",
    "h = layers.Dense(emb_dim, kernel_regularizer=regularizers.l2(0.0000))\n",
    "def one_hop(u, A):\n",
    "    C, _ = emb_sent_bow(inp_story)\n",
    "    x = layers.Reshape((1, emb_dim))(u)\n",
    "    x = layers.Dot(axes=2)([A, x])\n",
    "    x = layers.Reshape((story_maxsents,))(x)\n",
    "    x = layers.Activation('softmax')(x)\n",
    "    match = layers.Reshape((story_maxsents,1))(x)\n",
    "\n",
    "    x = layers.Dot(axes=1)([match, C])\n",
    "    x = layers.Reshape((emb_dim,))(x)\n",
    "    x = h(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Add()([x, emb_q])\n",
    "    return x, C\n",
    "response, emb_story = one_hop(emb_q, emb_story)\n",
    "response, emb_story = one_hop(response, emb_story)\n",
    "# response, emb_story = one_hop(response, emb_story)\n",
    "res = layers.Dense(vocab_size, activation='softmax')(response)\n",
    "answer = Model([inp_story, inp_q], res)\n",
    "answer.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "K.set_value(answer.optimizer.lr, 5e-3)\n",
    "hist=answer.fit(inps, answers_train, nb_epoch=100, batch_size=32,\n",
    "           validation_data=(val_inps, answers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
